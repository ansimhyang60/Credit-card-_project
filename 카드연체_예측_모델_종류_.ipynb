{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansimhyang60/mini_project/blob/main/%EC%B9%B4%EB%93%9C%EC%97%B0%EC%B2%B4_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8_%EC%A2%85%EB%A5%98_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRul7Y8iDywX"
      },
      "source": [
        "# 칼럼별 의미\n",
        "- gender: 성별\n",
        "- car: 차량 소유 여부\n",
        "- reality: 부동산 소유 여부\n",
        "- child_num: 자녀 수\n",
        "- income_total: 연간 소득\n",
        "- income_type: 소득 분류\n",
        "\t\t\t\t\t\t\t['Commercial associate', 'Working', 'State servant', 'Pensioner', 'Student']\n",
        "\n",
        "- edu_type: 교육 수준\n",
        "\t\t\t\t\t\t\t['Higher education' ,'Secondary / secondary special', 'Incomplete higher', 'Lower secondary', 'Academic degree']\n",
        "\n",
        "- family_type: 결혼 여부\n",
        "\t\t\t\t\t\t\t['Married', 'Civil marriage', 'Separated', 'Single / not married', 'Widow']\n",
        "\n",
        "- house_type: 생활 방식\n",
        "\t\t\t\t\t\t\t['Municipal apartment', 'House / apartment', 'With parents',\n",
        "\n",
        "\t\t\t\t\t\t\t 'Co-op apartment', 'Rented apartment', 'Office apartment']\n",
        "\n",
        "- DAYS_BIRTH: 출생일\n",
        "\t\t\t\t\t\t\t데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전에 태어났음을 의미\n",
        "\n",
        "- DAYS_EMPLOYED: 업무 시작일\n",
        "\t\t\t\t\t\t\t데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전부터 일을 시작함을 의미\n",
        "\n",
        "\t\t\t\t\t\t\t양수 값은 고용되지 않은 상태를 의미함\n",
        "\n",
        "- FLAG_MOBIL: 핸드폰 소유 여부\n",
        "- work_phone: 업무용 전화 소유 여부\n",
        "- phone: 전화 소유 여부\n",
        "- email: 이메일 소유 여부\n",
        "- occyp_type: 직업 유형\n",
        "- family_size: 가족 규모\n",
        "- begin_month: 신용카드 발급 월\n",
        "\t\t\t\t\t\t\t데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 한 달 전에 신용카드를 발급함을 의미\n",
        "\n",
        "- credit: 사용자의 신용카드 대금 연체를 기준으로 한 신용도\n",
        "\t\t\t\t\t\t\t=> 낮을 수록 높은 신용의 신용카드 사용자를 의미함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1YHCMMW-WBQ"
      },
      "outputs": [],
      "source": [
        "# !pip install catboost\n",
        "# !pip install category_encoders\n",
        "# !pip install pytorch_tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXBaJ7Sa_F0W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings, random\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from category_encoders.ordinal import OrdinalEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from typing import Dict, Tuple, Union, List\n",
        "import torch\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6GtMIU6AuSG"
      },
      "outputs": [],
      "source": [
        "def category_income(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  data[\"income_total\"] = data[\"income_total\"] / 10000\n",
        "  conditions = [\n",
        "    (data[\"income_total\"].le(18)),\n",
        "    (data[\"income_total\"].gt(18) & data[\"income_total\"].le(33)),\n",
        "    (data[\"income_total\"].gt(33) & data[\"income_total\"].le(49)),\n",
        "    (data[\"income_total\"].gt(49) & data[\"income_total\"].le(64)),\n",
        "    (data[\"income_total\"].gt(64) & data[\"income_total\"].le(80)),\n",
        "    (data[\"income_total\"].gt(80) & data[\"income_total\"].le(95)),\n",
        "    (data[\"income_total\"].gt(95) & data[\"income_total\"].le(111)),\n",
        "    (data[\"income_total\"].gt(111) & data[\"income_total\"].le(126)),\n",
        "    (data[\"income_total\"].gt(126) & data[\"income_total\"].le(142)),\n",
        "    (data[\"income_total\"].gt(142)),\n",
        "  ]\n",
        "  choices = [i for i in range(10)]\n",
        "\n",
        "  data[\"income_total\"] = np.select(conditions, choices)\n",
        "  return data\n",
        "\n",
        "def load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  train=pd.read_csv('/content/drive/MyDrive/CSV/train.csv')\n",
        "  test=pd.read_csv('/content/drive/MyDrive/CSV/test.csv')\n",
        "  #결측치 처리\n",
        "  train.fillna('NaN', inplace=True)\n",
        "  test.fillna('NaN', inplace=True)\n",
        "  #이상치 처리\n",
        "  train = train[(train['family_size'] <= 7)]\n",
        "  train = train.reset_index(drop=True)\n",
        "  #의미없는 변수 제거\n",
        "  train.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
        "  test.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
        "  train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'].map(lambda x: 0 if x > 0 else x)\n",
        "  test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'].map(lambda x: 0 if x > 0 else x)\n",
        "  feats = ['DAYS_BIRTH', 'begin_month', 'DAYS_EMPLOYED']\n",
        "  for feat in feats:\n",
        "    train[feat]=np.abs(train[feat])\n",
        "    test[feat]=np.abs(test[feat])\n",
        "\n",
        "  #파생변수 생성\n",
        "  for df in [train,test]:\n",
        "    # before_EMPLOYED: 고용되기 전까지의 일수\n",
        "    df['before_EMPLOYED'] = df['DAYS_BIRTH'] - df['DAYS_EMPLOYED']\n",
        "    df['income_total_befofeEMP_ratio'] = df['income_total'] / df['before_EMPLOYED']\n",
        "    df['before_EMPLOYED_m'] = np.floor(df['before_EMPLOYED'] / 30) - ((np.floor(df['before_EMPLOYED'] / 30) / 12).astype(int) * 12)\n",
        "    df['before_EMPLOYED_w'] = np.floor(df['before_EMPLOYED'] / 7) - ((np.floor(df['before_EMPLOYED'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "    #DAYS_BIRTH 파생변수- Age(나이), 태어난 월, 태어난 주(출생연도의 n주차)\n",
        "    df['Age'] = df['DAYS_BIRTH'] // 365\n",
        "    df['DAYS_BIRTH_m'] = np.floor(df['DAYS_BIRTH'] / 30) - ((np.floor(df['DAYS_BIRTH'] / 30) / 12).astype(int) * 12)\n",
        "    df['DAYS_BIRTH_w'] = np.floor(df['DAYS_BIRTH'] / 7) - ((np.floor(df['DAYS_BIRTH'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "\n",
        "    #DAYS_EMPLOYED_m 파생변수- EMPLOYED(근속연수), DAYS_EMPLOYED_m(고용된 달) ,DAYS_EMPLOYED_w(고용된 주(고용연도의 n주차))\n",
        "    df['EMPLOYED'] = df['DAYS_EMPLOYED'] // 365\n",
        "    df['DAYS_EMPLOYED_m'] = np.floor(df['DAYS_EMPLOYED'] / 30) - ((np.floor(df['DAYS_EMPLOYED'] / 30) / 12).astype(int) * 12)\n",
        "    df['DAYS_EMPLOYED_w'] = np.floor(df['DAYS_EMPLOYED'] / 7) - ((np.floor(df['DAYS_EMPLOYED'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "    #ability: 소득/(살아온 일수+ 근무일수)\n",
        "    df['ability'] = df['income_total'] / (df['DAYS_BIRTH'] + df['DAYS_EMPLOYED'])\n",
        "\n",
        "    #income_mean: 소득/ 가족 수\n",
        "    df['income_mean'] = df['income_total'] / df['family_size']\n",
        "\n",
        "    #ID 생성: 각 컬럼의 값들을 더해서 고유한 사람을 파악(*한 사람이 여러 개 카드를 만들 가능성을 고려해 begin_month는 제외함)\n",
        "    df['ID'] = \\\n",
        "    df['child_num'].astype(str) + '_' + df['income_total'].astype(str) + '_' +\\\n",
        "    df['DAYS_BIRTH'].astype(str) + '_' + df['DAYS_EMPLOYED'].astype(str) + '_' +\\\n",
        "    df['work_phone'].astype(str) + '_' + df['phone'].astype(str) + '_' +\\\n",
        "    df['email'].astype(str) + '_' + df['family_size'].astype(str) + '_' +\\\n",
        "    df['gender'].astype(str) + '_' + df['car'].astype(str) + '_' +\\\n",
        "    df['reality'].astype(str) + '_' + df['income_type'].astype(str) + '_' +\\\n",
        "    df['edu_type'].astype(str) + '_' + df['family_type'].astype(str) + '_' +\\\n",
        "    df['house_type'].astype(str) + '_' + df['occyp_type'].astype(str)\n",
        "\n",
        "  cols = ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED',]\n",
        "  train.drop(cols, axis=1, inplace=True)\n",
        "  test.drop(cols, axis=1, inplace=True)\n",
        "\n",
        "  numerical_feats = train.dtypes[train.dtypes != \"object\"].index.tolist()\n",
        "  numerical_feats.remove('credit')\n",
        "  print(\"Number of Numerical features: \", len(numerical_feats))\n",
        "\n",
        "  categorical_feats = train.dtypes[train.dtypes == \"object\"].index.tolist()\n",
        "  print(\"Number of Categorical features: \", len(categorical_feats))\n",
        "\n",
        "  for df in [train,test]:\n",
        "    df['income_total'] = np.log1p(1+df['income_total'])\n",
        "\n",
        "  encoder = OrdinalEncoder(categorical_feats)\n",
        "  train[categorical_feats] = encoder.fit_transform(train[categorical_feats], train['credit'])\n",
        "  test[categorical_feats] = encoder.transform(test[categorical_feats])\n",
        "\n",
        "  train['ID'] = train['ID'].astype('int64')\n",
        "  test['ID'] = test['ID'].astype('int64')\n",
        "\n",
        "  #클러스터링 구성\n",
        "  kmeans_train = train.drop(['credit'], axis=1)\n",
        "  kmeans_train = kmeans_train[numerical_feats]\n",
        "  kmeans = KMeans(n_clusters=36, random_state=42).fit(kmeans_train)\n",
        "  train['cluster'] = kmeans.predict(kmeans_train)\n",
        "  test['cluster'] = kmeans.predict(test[numerical_feats])\n",
        "\n",
        "  numerical_feats.remove('income_total')\n",
        "  scaler = StandardScaler()\n",
        "  train[numerical_feats] = scaler.fit_transform(train[numerical_feats])\n",
        "  test[numerical_feats] = scaler.transform(test[numerical_feats])\n",
        "\n",
        "  #모델링 catboost\n",
        "  n_est = 2000\n",
        "  seed = 42\n",
        "  n_fold = 15\n",
        "  n_class = 3\n",
        "\n",
        "  target = 'credit'\n",
        "  X = train.drop(target, axis=1)\n",
        "  y = train[target]\n",
        "  X_test = test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YJuxa7aCF68",
        "outputId": "f29b6777-928a-483a-aeed-1a9f520ab521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['work_phone',\n",
              " 'phone',\n",
              " 'email',\n",
              " 'family_size',\n",
              " 'begin_month',\n",
              " 'before_EMPLOYED',\n",
              " 'income_total_befofeEMP_ratio',\n",
              " 'before_EMPLOYED_m',\n",
              " 'before_EMPLOYED_w',\n",
              " 'before_EMPLOYED_w1',\n",
              " 'Age',\n",
              " 'DAYS_BIRTH_m',\n",
              " 'DAYS_BIRTH_w',\n",
              " 'DAYS_BIRTH_w1',\n",
              " 'EMPLOYED',\n",
              " 'DAYS_EMPLOYED_m',\n",
              " 'DAYS_EMPLOYED_w',\n",
              " 'DAYS_EMPLOYED_w1',\n",
              " 'ability',\n",
              " 'income_mean']"
            ]
          },
          "metadata": {},
          "execution_count": 392
        }
      ],
      "source": [
        "numerical_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDLn5r04CJqS",
        "outputId": "0378e803-4f79-45b3-a94a-492fb482c9ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gender',\n",
              " 'car',\n",
              " 'reality',\n",
              " 'income_type',\n",
              " 'edu_type',\n",
              " 'family_type',\n",
              " 'house_type',\n",
              " 'occyp_type',\n",
              " 'ID']"
            ]
          },
          "metadata": {},
          "execution_count": 393
        }
      ],
      "source": [
        "categorical_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cat_load_dataset()"
      ],
      "metadata": {
        "id": "2vUYowLRjoHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  train=pd.read_csv('/content/drive/MyDrive/CSV/train.csv')\n",
        "  test=pd.read_csv('/content/drive/MyDrive/CSV/test.csv')\n",
        "  #결측치 처리\n",
        "  train.fillna('NaN', inplace=True)\n",
        "  test.fillna('NaN', inplace=True)\n",
        "  #이상치 처리\n",
        "  train = train[(train['family_size'] <= 7)]\n",
        "  train = train.reset_index(drop=True)\n",
        "  #의미없는 변수 제거\n",
        "  train.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
        "  test.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
        "  train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'].map(lambda x: 0 if x > 0 else x)\n",
        "  test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'].map(lambda x: 0 if x > 0 else x)\n",
        "  feats = ['DAYS_BIRTH', 'begin_month', 'DAYS_EMPLOYED']\n",
        "  for feat in feats:\n",
        "    train[feat]=np.abs(train[feat])\n",
        "    test[feat]=np.abs(test[feat])\n",
        "\n",
        "  #income_ total\n",
        "  train = category_income(train)\n",
        "  test = category_income(test)\n",
        "\n",
        "  #파생변수 생성\n",
        "  for df in [train,test]:\n",
        "    # before_EMPLOYED: 고용되기 전까지의 일수\n",
        "    df['before_EMPLOYED'] = df['DAYS_BIRTH'] - df['DAYS_EMPLOYED']\n",
        "    df['income_total_befofeEMP_ratio'] = df['income_total'] / df['before_EMPLOYED']\n",
        "    df['before_EMPLOYED_m'] = np.floor(df['before_EMPLOYED'] / 30) - ((np.floor(df['before_EMPLOYED'] / 30) / 12).astype(int) * 12)\n",
        "    df['before_EMPLOYED_w'] = np.floor(df['before_EMPLOYED'] / 7) - ((np.floor(df['before_EMPLOYED'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "    #DAYS_BIRTH 파생변수- Age(나이), 태어난 월, 태어난 주(출생연도의 n주차)\n",
        "    df['Age'] = df['DAYS_BIRTH'] // 365\n",
        "    df['DAYS_BIRTH_m'] = np.floor(df['DAYS_BIRTH'] / 30) - ((np.floor(df['DAYS_BIRTH'] / 30) / 12).astype(int) * 12)\n",
        "    df['DAYS_BIRTH_w'] = np.floor(df['DAYS_BIRTH'] / 7) - ((np.floor(df['DAYS_BIRTH'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "\n",
        "    #DAYS_EMPLOYED_m 파생변수- EMPLOYED(근속연수), DAYS_EMPLOYED_m(고용된 달) ,DAYS_EMPLOYED_w(고용된 주(고용연도의 n주차))\n",
        "    df['EMPLOYED'] = df['DAYS_EMPLOYED'] // 365\n",
        "    df['DAYS_EMPLOYED_m'] = np.floor(df['DAYS_EMPLOYED'] / 30) - ((np.floor(df['DAYS_EMPLOYED'] / 30) / 12).astype(int) * 12)\n",
        "    df['DAYS_EMPLOYED_w'] = np.floor(df['DAYS_EMPLOYED'] / 7) - ((np.floor(df['DAYS_EMPLOYED'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "    #ability: 소득/(살아온 일수+ 근무일수)\n",
        "    df['ability'] = df['income_total'] / (df['DAYS_BIRTH'] + df['DAYS_EMPLOYED'])\n",
        "\n",
        "    #income_mean: 소득/ 가족 수\n",
        "    df['income_mean'] = df['income_total'] / df['family_size']\n",
        "\n",
        "    #ID 생성: 각 컬럼의 값들을 더해서 고유한 사람을 파악(*한 사람이 여러 개 카드를 만들 가능성을 고려해 begin_month는 제외함)\n",
        "    df['ID'] = \\\n",
        "    df['child_num'].astype(str) + '_' + df['income_total'].astype(str) + '_' +\\\n",
        "    df['DAYS_BIRTH'].astype(str) + '_' + df['DAYS_EMPLOYED'].astype(str) + '_' +\\\n",
        "    df['work_phone'].astype(str) + '_' + df['phone'].astype(str) + '_' +\\\n",
        "    df['email'].astype(str) + '_' + df['family_size'].astype(str) + '_' +\\\n",
        "    df['gender'].astype(str) + '_' + df['car'].astype(str) + '_' +\\\n",
        "    df['reality'].astype(str) + '_' + df['income_type'].astype(str) + '_' +\\\n",
        "    df['edu_type'].astype(str) + '_' + df['family_type'].astype(str) + '_' +\\\n",
        "    df['house_type'].astype(str) + '_' + df['occyp_type'].astype(str)\n",
        "\n",
        "  cols = ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED',]\n",
        "  train.drop(cols, axis=1, inplace=True)\n",
        "  test.drop(cols, axis=1, inplace=True)\n",
        "\n",
        "  numerical_feats = train.dtypes[train.dtypes != \"object\"].index.tolist()\n",
        "  numerical_feats.remove('credit')\n",
        "  print(\"Number of Numerical features: \", len(numerical_feats))\n",
        "\n",
        "  categorical_feats = train.dtypes[train.dtypes == \"object\"].index.tolist()\n",
        "  print(\"Number of Categorical features: \", len(categorical_feats))\n",
        "\n",
        "  for df in [train,test]:\n",
        "    df['income_total'] = np.log1p(1+df['income_total'])\n",
        "\n",
        "  encoder = OrdinalEncoder(categorical_feats)\n",
        "  train[categorical_feats] = encoder.fit_transform(train[categorical_feats], train['credit'])\n",
        "  test[categorical_feats] = encoder.transform(test[categorical_feats])\n",
        "\n",
        "  train['ID'] = train['ID'].astype('int64')\n",
        "  test['ID'] = test['ID'].astype('int64')\n",
        "\n",
        "  #클러스터링 구성\n",
        "  kmeans_train = train.drop(['credit'], axis=1)\n",
        "  kmeans_train = kmeans_train[numerical_feats]\n",
        "  kmeans = KMeans(n_clusters=36, random_state=42).fit(kmeans_train)\n",
        "  train['cluster'] = kmeans.predict(kmeans_train)\n",
        "  test['cluster'] = kmeans.predict(test[numerical_feats])\n",
        "\n",
        "  numerical_feats.remove('income_total')\n",
        "  scaler = StandardScaler()\n",
        "  train[numerical_feats] = scaler.fit_transform(train[numerical_feats])\n",
        "  test[numerical_feats] = scaler.transform(test[numerical_feats])\n",
        "\n",
        "  #모델링 catboost\n",
        "  n_est = 2000\n",
        "  seed = 42\n",
        "  n_fold = 15\n",
        "  n_class = 3\n",
        "\n",
        "  target = 'credit'\n",
        "  X = train.drop(target, axis=1)\n",
        "  y = train[target]\n",
        "  X_test = test"
      ],
      "metadata": {
        "id": "91frMNv6jh13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StratifiedKFold 기본모델, 성능은 0.665208"
      ],
      "metadata": {
        "id": "DUoU8XmP9K7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "m68ILH4gsd4g",
        "outputId": "ccc14e99-5427-4141-d75d-762302ec1b48"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-396-4bdfc8c94e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mskfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \"\"\"\n\u001b[1;32m    330\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'NoneType'>"
          ]
        }
      ],
      "source": [
        "skfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
        "folds=[]\n",
        "for train_idx, valid_idx in skfold.split(X, y):\n",
        "        folds.append((train_idx, valid_idx))\n",
        "\n",
        "cat_pred = np.zeros((X.shape[0], n_class))\n",
        "cat_pred_test = np.zeros((X_test.shape[0], n_class))\n",
        "cat_cols = ['income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type', 'ID']\n",
        "for fold in range(n_fold):\n",
        "  print(f'\\n----------------- Fold {fold} -----------------\\n')\n",
        "  train_idx, valid_idx = folds[fold]\n",
        "  X_train, X_valid, y_train, y_valid = X.iloc[train_idx], X.iloc[valid_idx], y[train_idx], y[valid_idx]\n",
        "  train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n",
        "  valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols)\n",
        "\n",
        "  model_cat = CatBoostClassifier(n_estimators=n_est, random_seed=seed)  #n_estimators=n_est, random_seed=seed  랜덤시드 고정\n",
        "  model_cat.fit(train_data, eval_set=valid_data, use_best_model=True, early_stopping_rounds=210, verbose=100)\n",
        "\n",
        "  cat_pred[valid_idx] = model_cat.predict_proba(X_valid)\n",
        "  cat_pred_test += model_cat.predict_proba(X_test) / n_fold\n",
        "  print(f'CV Log Loss Score: {log_loss(y_valid, cat_pred[valid_idx]):.6f}')\n",
        "\n",
        "print(f'\\tLog Loss: {log_loss(y, cat_pred):.6f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stratified_kfold_cat 모델"
      ],
      "metadata": {
        "id": "_pHmk6tH9bKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfAI1Qc72pZZ"
      },
      "outputs": [],
      "source": [
        "# Catboost\n",
        "def stratified_kfold_cat(\n",
        "    params: Dict[str, Union[int, float, str, List[str]]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    cat_oof = np.zeros((X.shape[0], 3))\n",
        "    cat_preds = np.zeros((X_test.shape[0], 3))\n",
        "    cat_cols = [c for c in X.columns if X[c].dtypes == \"int64\"]\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n",
        "        valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols)\n",
        "\n",
        "        model = CatBoostClassifier(**params, n_estimators=2000, random_seed=42 )\n",
        "\n",
        "        model.fit(\n",
        "            train_data,\n",
        "            eval_set=valid_data,\n",
        "            early_stopping_rounds=210,\n",
        "            use_best_model=True,\n",
        "            verbose=100,\n",
        "        )\n",
        "\n",
        "        cat_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        cat_preds += model.predict_proba(X_test) / n_fold\n",
        "\n",
        "    log_score = log_loss(y, cat_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\\n\")\n",
        "    return cat_oof, cat_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cat Train\n",
        "- cat 하이퍼파라미터튜닝(optuna 라이브러리)"
      ],
      "metadata": {
        "id": "44nRu2bbHP-F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkFCtUU3cvy4"
      },
      "outputs": [],
      "source": [
        "# train_cat = cat_load_dataset()\n",
        "# test_cat = cat_load_dataset()\n",
        "# X = train_cat\n",
        "# y = train_cat[\"credit\"]\n",
        "# X_test = test_cat.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJpckFPFB3lM"
      },
      "source": [
        "# catboost 성능: log loss score: 0.66861\n",
        "- 성능 낮음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCZ64pNg5hkO"
      },
      "outputs": [],
      "source": [
        "cat_params = {\n",
        "    \"learning_rate\": 0.026612467217016746,\n",
        "    \"l2_leaf_reg\": 0.3753065117824262,\n",
        "    \"max_depth\": 8,\n",
        "    \"bagging_temperature\": 1,\n",
        "    \"min_data_in_leaf\": 57,\n",
        "    \"max_bin\": 494,\n",
        "    \"random_state\": 42,\n",
        "    \"eval_metric\": \"MultiClass\",\n",
        "    \"loss_function\": \"MultiClass\",\n",
        "    \"od_type\": \"Iter\",\n",
        "    \"od_wait\": 500,\n",
        "    \"iterations\": 10000,\n",
        "    \"cat_features\": [\n",
        "       'gender',\n",
        "       'car',\n",
        "       'reality',\n",
        "       'income_type',\n",
        "       'edu_type',\n",
        "       'family_type',\n",
        "       'house_type',\n",
        "       'occyp_type',\n",
        "       'ID',],\n",
        "}\n",
        "\n",
        "cat_oof, cat_preds = stratified_kfold_cat(cat_params, 10, X, y, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Light GBM"
      ],
      "metadata": {
        "id": "eF1u4UDfJQm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3aTuaE64id1"
      },
      "outputs": [],
      "source": [
        "# Light GBM\n",
        "def stratified_kfold_lgbm(\n",
        "    params: Dict[str, Union[int, float, str]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    lgb_oof = np.zeros((X.shape[0], 3))\n",
        "    lgb_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        pre_model = LGBMClassifier(**params)\n",
        "\n",
        "        pre_model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=100,\n",
        "        )\n",
        "        params2 = params.copy()\n",
        "        params2[\"learning_rate\"] = params[\"learning_rate\"] * 0.1\n",
        "\n",
        "        model = LGBMClassifier(**params2)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=100,\n",
        "            init_model=pre_model,\n",
        "        )\n",
        "        lgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        lgb_preds += model.predict_proba(X_test) / n_fold\n",
        "\n",
        "    log_score = log_loss(y, lgb_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
        "\n",
        "    return lgb_oof, lgb_preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Light GBM Train(optuna 사용) 모델평과: 0.66409\n",
        "- 성능 가장 높음"
      ],
      "metadata": {
        "id": "Nn90BXz8Fdcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = load_dataset()\n",
        "X = train.drop(\"credit\", axis=1)\n",
        "y = train[\"credit\"]\n",
        "X_test = test.copy()"
      ],
      "metadata": {
        "id": "6OKnONyXIlK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI20u1SGGxBx"
      },
      "outputs": [],
      "source": [
        "lgb_params = {\n",
        "    \"eta\": 0.023839252347297356,\n",
        "    \"reg_alpha\": 5.998770177220496e-05,\n",
        "    \"reg_lambda\": 0.07127674208132959,\n",
        "    \"max_depth\": 18,\n",
        "    \"num_leaves\": 125,\n",
        "    \"colsample_bytree\": 0.4241631237880101,\n",
        "    \"subsample\": 0.8876057928391585,\n",
        "    \"subsample_freq\": 5,\n",
        "    \"min_child_samples\": 5,\n",
        "    \"max_bin\": 449,\n",
        "    \"random_state\": 42,\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"n_estimators\": 10000,\n",
        "    \"objective\": \"multiclass\",\n",
        "    \"metric\": \"multi_logloss\",\n",
        "}\n",
        "lgbm_oof, lgbm_preds = stratified_kfold_lgbm(lgb_params, 10, X, y, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGB"
      ],
      "metadata": {
        "id": "r8ayPc6eJatv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zrTkRdf4pBO"
      },
      "outputs": [],
      "source": [
        "def stratified_kfold_xgb(\n",
        "    params: Dict[str, Union[int, float, str]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    xgb_oof = np.zeros((X.shape[0], 3))\n",
        "    xgb_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "        model = XGBClassifier(**params)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=100,\n",
        "        )\n",
        "\n",
        "        xgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        xgb_preds += model.predict_proba(X_test) / n_fold\n",
        "\n",
        "    log_score = log_loss(y, xgb_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
        "\n",
        "    return xgb_oof, xgb_preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGB Train"
      ],
      "metadata": {
        "id": "A5SyqLtVJvQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_params = {\n",
        "    \"eta\": 0.023839252347297356,\n",
        "    \"reg_alpha\": 6.99554614267605e-06,\n",
        "    \"reg_lambda\": 0.010419988953061583,\n",
        "    \"max_depth\": 15,\n",
        "    \"max_leaves\": 159,\n",
        "    \"colsample_bytree\": 0.4515469593932409,\n",
        "    \"subsample\": 0.7732694309118915,\n",
        "    \"min_child_weight\": 5,\n",
        "    \"gamma\": 0.6847131315687576,\n",
        "    \"random_state\": 42,\n",
        "    \"n_estimators\": 10000,\n",
        "    \"objective\": \"multi:softmax\",\n",
        "    \"eval_metric\": \"mlogloss\",\n",
        "}\n",
        "xgb_oof, xgb_preds = stratified_kfold_xgb(xgb_params, 10, X, y, X_test)"
      ],
      "metadata": {
        "id": "mQ9mbVm3Jso2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#랜덤포레스트"
      ],
      "metadata": {
        "id": "9Obo2XqwJ2gI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYZ8G7Hu4t5d"
      },
      "outputs": [],
      "source": [
        "#랜덤포레스트\n",
        "def stratified_kfold_rf(\n",
        "    params: Dict[str, Union[int, float, str, bool]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    rf_oof = np.zeros((X.shape[0], 3))\n",
        "    rf_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        model = RandomForestClassifier(**params)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "        )\n",
        "\n",
        "        rf_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        rf_preds += model.predict_proba(X_test) / n_fold\n",
        "        print(f\"Log Loss Score: {log_loss(y_valid, rf_oof[valid_idx]):.5f}\")\n",
        "\n",
        "    log_score = log_loss(y, rf_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
        "\n",
        "    return rf_oof, rf_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOE9cZbLG97F"
      },
      "outputs": [],
      "source": [
        "rf_params = {\n",
        "        \"criterion\": \"gini\",\n",
        "        \"n_estimators\": 300,\n",
        "        \"min_samples_split\": 10,\n",
        "        \"min_samples_leaf\": 2,\n",
        "        \"max_features\": \"auto\",\n",
        "        \"oob_score\": True,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "rf_oof, rf_preds = stratified_kfold_rf(rf_params, 10, X, y, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = load_dataset()\n",
        "train_x = train.drop(\"credit\", axis = 1)\n",
        "train_y = train['credit'].values"
      ],
      "metadata": {
        "id": "9MkcISx3KmOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred = np.concatenate([cat_oof, lgbm_oof, xgb_oof, rf_oof], axis=1)\n",
        "train_pred.shape"
      ],
      "metadata": {
        "id": "eMawyDc1KoUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = np.concatenate([cat_preds, lgbm_preds, xgb_preds, rf_preds], axis=1)\n",
        "test_pred.shape"
      ],
      "metadata": {
        "id": "rNZloAzFKodp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Tabular\n",
        "- Stacking Ensembel을 사용하여 학습진행"
      ],
      "metadata": {
        "id": "7nuxgmQuKLcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "dwr6juahJ-q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_fold = 10\n",
        "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "splits = folds.split(train_pred, train_y)\n",
        "net_oof = np.zeros((train_pred.shape[0], 3))\n",
        "net_preds = np.zeros((test_pred.shape[0], 3))\n",
        "for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "    print(f\"============ Fold {fold} ============\\n\")\n",
        "    X_train, X_valid = train_pred[train_idx], train_pred[valid_idx]\n",
        "    y_train, y_valid = train_y[train_idx], train_y[valid_idx]\n",
        "    model = TabNetMultiTaskClassifier(\n",
        "            n_d=64, n_a=64, n_steps=1,\n",
        "            lambda_sparse=1e-4,\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params=dict(lr=2e-2),\n",
        "            scheduler_params = {\"gamma\": 0.9, \"step_size\": 50},\n",
        "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "            mask_type=\"entmax\",\n",
        "            device_name=device\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train.reshape(-1,1),\n",
        "        eval_set=[(X_valid, y_valid.reshape(-1,1))],\n",
        "        max_epochs=100,\n",
        "        batch_size=1024,\n",
        "        eval_metric=[\"logloss\"],\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=1,\n",
        "        drop_last=False\n",
        "    )\n",
        "    net_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "    net_preds += model.predict_proba(test_pred)[0] / n_fold\n",
        "log_score = log_loss(train_y, net_oof)\n",
        "print(f\"Log Loss Score: {log_score:.5f}\")"
      ],
      "metadata": {
        "id": "dlNSzqBNkj6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(path+\"sample_submission.csv\")\n",
        "submission.iloc[:, 1:] = net_preds\n",
        "submission.to_csv(\"meta_ensemble_submit.csv\", index=False)"
      ],
      "metadata": {
        "id": "lZfdCAPHkzLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 생략하기"
      ],
      "metadata": {
        "id": "JRSNpvXpL57g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tkWdH8isjRn"
      },
      "outputs": [],
      "source": [
        "# def plot_feature_importance(importance,names,model_type):\n",
        "\n",
        "#     feature_importance = np.array(importance)\n",
        "#     feature_names = np.array(names)\n",
        "\n",
        "#     data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
        "#     fi_df = pd.DataFrame(data)\n",
        "\n",
        "#     fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
        "\n",
        "#     plt.figure(figsize=(10,8))\n",
        "\n",
        "#     sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
        "\n",
        "#     plt.title(model_type + ' Feature Importance')\n",
        "#     plt.xlabel('Feature Importance')\n",
        "#     plt.ylabel('Feature Names')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50uErwuHsmyS"
      },
      "outputs": [],
      "source": [
        "# plot_feature_importance(model_cat.get_feature_importance(),X_test.columns,'CATBOOST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuEBb1iqEyoU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDybYJ-95O2M"
      },
      "outputs": [],
      "source": [
        "# fig, ax=plt.subplots() #히트맵, annot=true 컬럼명 출력\n",
        "# fig.set_size_inches(20,10)\n",
        "# sns.heatmap(train.corr(), annot=True, cmap='RdBu_r')  #각각의 공간에 숫자를 넣는 것을 annot=Ture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJu_4Gbo5kNr"
      },
      "outputs": [],
      "source": [
        "# fig, ax=plt.subplots() #히트맵, annot=true 컬럼명 출력\n",
        "# fig.set_size_inches(20,10)\n",
        "# sns.heatmap(test.corr(), annot=True, cmap='RdBu_r')  #각각의 공간에 숫자를 넣는 것을 annot=Ture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWSYkEsy8reG"
      },
      "outputs": [],
      "source": [
        "# import matplotlib as mpl\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from scipy import stats\n",
        "\n",
        "# %matplotlib inline\n",
        "# #그래프가 출려되지 않은 오류를 대처하기 위한 코드\n",
        "\n",
        "# plt.style.use('ggplot')#그래프에서 범위를 격자로 보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAcf2rJA7zex"
      },
      "outputs": [],
      "source": [
        "# plt.subplots(figsize = (8,8))\n",
        "# plt.pie(train['credit'].value_counts(),labels = train['credit'].value_counts().index,\n",
        "# autopct=\"%.2f%%\", shadow = True, startangle=90) # 비율 시작이 12시부터 시작.\n",
        "# plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIPiRIYxJrfW"
      },
      "source": [
        "# train, test 합친 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CuQYBxL-fJb"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# data_pension=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/머신러닝프로젝트2조/CSV/data-Pension.csv')\n",
        "# data_pension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpaXYt8rJAJD"
      },
      "outputs": [],
      "source": [
        "# data_pension.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdlTEWhCJoFj"
      },
      "outputs": [],
      "source": [
        "# data_pension.fillna('NaN', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mto8CouMJoFs"
      },
      "outputs": [],
      "source": [
        "# data_pension = data_pension[(data_pension['family_size'] <= 7)]\n",
        "# data_pension = data_pension.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPakbQxMf2BM"
      },
      "outputs": [],
      "source": [
        "# data_pension.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mhh4k7kJoFs"
      },
      "outputs": [],
      "source": [
        "# data_pension.drop(['FLAG_MOBIL', 'index', 'index-old'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQxG2a5fJoFs"
      },
      "outputs": [],
      "source": [
        "# data_pension['DAYS_EMPLOYED'] = data_pension['DAYS_EMPLOYED'].map(lambda x: 0 if x > 0 else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpaQ8_tiJoFs"
      },
      "outputs": [],
      "source": [
        "# feats = ['DAYS_BIRTH', 'begin_month', 'DAYS_EMPLOYED']\n",
        "# for feat in feats:\n",
        "#     data_pension[feat]=np.abs(data_pension[feat])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSLzj3bjJoFs"
      },
      "outputs": [],
      "source": [
        "# for df in [data_pension]:\n",
        "#     # before_EMPLOYED: 고용되기 전까지의 일수\n",
        "#     df['before_EMPLOYED'] = df['DAYS_BIRTH'] - df['DAYS_EMPLOYED']\n",
        "#     df['income_total_befofeEMP_ratio'] = df['income_total'] / df['before_EMPLOYED']\n",
        "#     df['before_EMPLOYED_m'] = np.floor(df['before_EMPLOYED'] / 30) - ((np.floor(df['before_EMPLOYED'] / 30) / 12).astype(int) * 12)\n",
        "#     df['before_EMPLOYED_w'] = np.floor(df['before_EMPLOYED'] / 7) - ((np.floor(df['before_EMPLOYED'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "#     #DAYS_BIRTH 파생변수- Age(나이), 태어난 월, 태어난 주(출생연도의 n주차)\n",
        "#     df['Age'] = df['DAYS_BIRTH'] // 365\n",
        "#     df['DAYS_BIRTH_m'] = np.floor(df['DAYS_BIRTH'] / 30) - ((np.floor(df['DAYS_BIRTH'] / 30) / 12).astype(int) * 12)\n",
        "#     df['DAYS_BIRTH_w'] = np.floor(df['DAYS_BIRTH'] / 7) - ((np.floor(df['DAYS_BIRTH'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "\n",
        "#     #DAYS_EMPLOYED_m 파생변수- EMPLOYED(근속연수), DAYS_EMPLOYED_m(고용된 달) ,DAYS_EMPLOYED_w(고용된 주(고용연도의 n주차))\n",
        "#     df['EMPLOYED'] = df['DAYS_EMPLOYED'] // 365\n",
        "#     df['DAYS_EMPLOYED_m'] = np.floor(df['DAYS_EMPLOYED'] / 30) - ((np.floor(df['DAYS_EMPLOYED'] / 30) / 12).astype(int) * 12)\n",
        "#     df['DAYS_EMPLOYED_w'] = np.floor(df['DAYS_EMPLOYED'] / 7) - ((np.floor(df['DAYS_EMPLOYED'] / 7) / 4).astype(int) * 4)\n",
        "\n",
        "#     #ability: 소득/(살아온 일수+ 근무일수)\n",
        "#     df['ability'] = df['income_total'] / (df['DAYS_BIRTH'] + df['DAYS_EMPLOYED'])\n",
        "\n",
        "#     #income_mean: 소득/ 가족 수\n",
        "#     df['income_mean'] = df['income_total'] / df['family_size']\n",
        "\n",
        "#     #ID 생성: 각 컬럼의 값들을 더해서 고유한 사람을 파악(*한 사람이 여러 개 카드를 만들 가능성을 고려해 begin_month는 제외함)\n",
        "#     df['ID'] = \\\n",
        "#     df['child_num'].astype(str) + '_' + df['income_total'].astype(str) + '_' +\\\n",
        "#     df['DAYS_BIRTH'].astype(str) + '_' + df['DAYS_EMPLOYED'].astype(str) + '_' +\\\n",
        "#     df['work_phone'].astype(str) + '_' + df['phone'].astype(str) + '_' +\\\n",
        "#     df['email'].astype(str) + '_' + df['family_size'].astype(str) + '_' +\\\n",
        "#     df['gender'].astype(str) + '_' + df['car'].astype(str) + '_' +\\\n",
        "#     df['reality'].astype(str) + '_' + df['income_type'].astype(str) + '_' +\\\n",
        "#     df['edu_type'].astype(str) + '_' + df['family_type'].astype(str) + '_' +\\\n",
        "#     df['house_type'].astype(str) + '_' + df['occyp_type'].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tgywUUFJoFt"
      },
      "outputs": [],
      "source": [
        "# cols = ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED',]\n",
        "# data_pension.drop(cols, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVdZ1T5tJoFt"
      },
      "outputs": [],
      "source": [
        "# numerical_feats = data_pension.dtypes[data_pension.dtypes != \"object\"].index.tolist()\n",
        "# print(\"Number of Numerical features: \", len(numerical_feats))\n",
        "\n",
        "# categorical_feats = data_pension.dtypes[data_pension.dtypes == \"object\"].index.tolist()\n",
        "# # print(\"Number of Categorical features: \", len(categorical_feats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx8QseRfJoFt"
      },
      "outputs": [],
      "source": [
        "# numerical_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN_RPxQ9JoFt"
      },
      "outputs": [],
      "source": [
        "# categorical_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXCV1tWqJoFt"
      },
      "outputs": [],
      "source": [
        "# for df in []:\n",
        "#     df['income_total'] = np.log1p(1+df['income_total'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyA5BSnyvb9j"
      },
      "outputs": [],
      "source": [
        "# from category_encoders.ordinal import OrdinalEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZqqGEoT-pqC"
      },
      "outputs": [],
      "source": [
        "# data_pension['ID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeuPRw3dJoFt"
      },
      "outputs": [],
      "source": [
        "# encoder = OrdinalEncoder(categorical_feats)\n",
        "# data_pension[categorical_feats] = encoder.fit_transform(data_pension[categorical_feats], data_pension['credit'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EX4oC0JJoFt"
      },
      "outputs": [],
      "source": [
        "# data_pension['ID'] = data_pension['ID'].astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V72ucJ9-JoFt"
      },
      "outputs": [],
      "source": [
        "# kmeans_data_pension = data_pension.drop(['credit'], axis=1)\n",
        "# kmeans_data_pension=kmeans_data_pension[numerical_feats]\n",
        "# kmeans = KMeans(n_clusters=36, random_state=42).fit(kmeans_data_pension)\n",
        "# data_pension['cluster'] = kmeans.predict(kmeans_data_pension)\n",
        "# data_pension['cluster'] = kmeans.predict(data_pension[numerical_feats])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imk6EcQRJoFt"
      },
      "outputs": [],
      "source": [
        "# numerical_feats.remove('income_total')\n",
        "# scaler = StandardScaler()\n",
        "# data_pension[numerical_feats] = scaler.fit_transform(data_pension[numerical_feats])\n",
        "# data_pension[numerical_feats] = scaler.transform(data_pension[numerical_feats])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5iSTOsMJoFt"
      },
      "outputs": [],
      "source": [
        "# n_est = 2000\n",
        "# seed = 42\n",
        "# n_fold = 15\n",
        "# n_class = 3\n",
        "\n",
        "# target = 'credit'\n",
        "# X = data_pension.drop(target, axis=1)\n",
        "# y = data_pension[target]\n",
        "# X_test = test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-a-mHhlJoFt"
      },
      "outputs": [],
      "source": [
        "# skfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
        "# folds=[]\n",
        "# for data_pension_idx, valid_idx in skfold.split(X, y):\n",
        "#         folds.append((data_pension_idx, valid_idx))\n",
        "\n",
        "# cat_pred = np.zeros((X.shape[0], n_class))\n",
        "# cat_pred_test = np.zeros((X_test.shape[0], n_class))\n",
        "# cat_cols = ['income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type', 'ID']\n",
        "# for fold in range(n_fold):\n",
        "#   print(f'\\n----------------- Fold {fold} -----------------\\n')\n",
        "#   data_pension_idx, valid_idx = folds[fold]\n",
        "#   X_train, X_valid, y_train, y_valid = X.iloc[data_pension_idx], X.iloc[valid_idx], y[data_pension_idx], y[valid_idx]\n",
        "#   train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n",
        "#   valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols)\n",
        "\n",
        "#   model_cat = CatBoostClassifier()\n",
        "#   model_cat.fit(train_data, eval_set=valid_data, use_best_model=True, early_stopping_rounds=100, verbose=100)\n",
        "\n",
        "# #   cat_pred[valid_idx] = model_cat.predict_proba(X_valid)\n",
        "# #   cat_pred_test += model_cat.predict_proba(X_test) / n_fold\n",
        "# #   print(f'CV Log Loss Score: {log_loss(y_valid, cat_pred[valid_idx]):.6f}')\n",
        "\n",
        "# # print(f'\\tLog Loss: {log_loss(y, cat_pred):.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SLKdjvaJoFu"
      },
      "outputs": [],
      "source": [
        "# def plot_feature_importance(importance,names,model_type):\n",
        "\n",
        "#     feature_importance = np.array(importance)\n",
        "#     feature_names = np.array(names)\n",
        "\n",
        "#     data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
        "#     fi_df = pd.DataFrame(data)\n",
        "\n",
        "#     fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
        "\n",
        "#     plt.figure(figsize=(10,8))\n",
        "\n",
        "#     sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
        "\n",
        "#     plt.title(model_type + ' Feature Importance')\n",
        "#     plt.xlabel('Feature Importance')\n",
        "#     plt.ylabel('Feature Names')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv26w689JoFu"
      },
      "outputs": [],
      "source": [
        "# plot_feature_importance(model_cat.get_feature_importance(),X_test.columns,'CATBOOST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uislrgbUJoFu"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split #모델 만들 때 비율: 0.2(독립변수 80%, 20%), 종속변수(80%, 20%)\n",
        "# X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=156)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-JxulvaJoFu"
      },
      "outputs": [],
      "source": [
        "# # RandomForestRegressor: RandomForest 알고리즘을 이용해서 값을 예측(회귀)\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# rf_model=RandomForestRegressor(\n",
        "#     random_state=0 #같은 값을 설정하면 다음에 실행할 때 같으 값을 선택\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9q9DxRrJoFu"
      },
      "outputs": [],
      "source": [
        "# rf_model=RandomForestRegressor(\n",
        "#     random_state=0\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vovCTETZJoFu"
      },
      "outputs": [],
      "source": [
        "# rf_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHmA8HAAJoFu"
      },
      "outputs": [],
      "source": [
        "# rf_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7Sj0hIRJoFu"
      },
      "outputs": [],
      "source": [
        "# pred=rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckcza7f9JoFu"
      },
      "outputs": [],
      "source": [
        "# pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7zjoLFgJoFu"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7UyoaiKJoFu"
      },
      "outputs": [],
      "source": [
        "# print(\"MSE:\", mean_squared_error(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yox2k7TOJoFv"
      },
      "outputs": [],
      "source": [
        "# from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-hvx56yJoFv"
      },
      "outputs": [],
      "source": [
        "# model_xgb=XGBRegressor(\n",
        "#     n_estimators=150,\n",
        "#     max_depth=5,\n",
        "#     random_state=0,\n",
        "#     min_child_weight=7,\n",
        "#     learning_rate=0.1\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcIGaXRtJoFv"
      },
      "outputs": [],
      "source": [
        "# model_xgb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVNUheWHJoFv"
      },
      "outputs": [],
      "source": [
        "# pred=model_xgb.predict(X_test)\n",
        "# pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCWsISm6JoFv"
      },
      "outputs": [],
      "source": [
        "# print('MSE', mean_squared_error(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8CSFpGiPb_e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dIPiRIYxJrfW"
      ],
      "provenance": [],
      "mount_file_id": "1pMsOr6qRcSVackWktJymaTV14kYERroT",
      "authorship_tag": "ABX9TyMMZbPaxViDfkB/LNg+G9Ym",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}